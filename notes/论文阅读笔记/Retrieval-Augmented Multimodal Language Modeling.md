# ICML2023: Retrieval-Augmented Multimodal Language Modeling

- [论文连接](https://arxiv.org/abs/2211.12561v1)
- 代码连接
- 斯坦福大学,Meta,华盛顿大学
- arxiv v1 2022.11.11,v2 2023.6.6
- cite:102

## Kimi总结

这篇论文介绍了一种新型的多模态语言模型，名为Retrieval-Augmented CM3（RA-CM3），它能够通过检索增强的方式来生成图像和文本。以下是对论文内容的总结：

### 背景与动机

- 现有的多模态模型（如DALL-E和CM3）在文本到图像和图像到文本的生成任务中取得了显著进展，但这些模型需要大量的参数和训练数据来存储知识。
- 为了更可扩展和模块化地整合知识，论文提出了一种检索增强的多模态模型，该模型允许基础多模态模型（生成器）通过检索器从外部存储器（如网络上的文档）检索相关文本和图像。

### 方法

- **检索器（Retriever）**：使用预训练的CLIP模型作为检索器，从外部存储器中检索与输入多模态文档相关的文档。
- **生成器（Generator）**：基于CM3 Transformer架构，将检索到的文档作为上下文例子添加到主输入文档中，并训练生成器以优化令牌预测损失。
- **混合模态编码器（Mixed-modal Encoder）**：设计了一个能够编码文本和图像组合的混合模态编码器，用于检索器和生成器。

### 实验

- 使用LAION数据集训练模型，并在MS-COCO数据集上评估图像和标题生成任务。
- RA-CM3在图像和标题生成任务上显著优于基线模型（如DALL-E和CM3），同时训练所需的计算资源更少（不到DALL-E的30%）。

### 结果

- RA-CM3在MS-COCO图像和标题生成任务上的性能得到了显著提升，同时在训练效率上也优于现有工作。
- 论文还展示了RA-CM3的新能力，如忠实的图像生成和多模态上下文学习（例如，通过上下文示例控制图像生成）。

### 结论

- 论文提出了一种通用且模块化的检索增强框架，用于多模态模型，这为未来的研究开辟了新的方向，如改进多模态检索器和生成器，扩展模态超出图像和文本，以及进一步研究多模态提示和上下文学习。

### 伦理和社会影响

- 论文讨论了多模态生成模型可能存在的偏见和不安全内容问题，并强调了在训练数据中进行过滤以减少这些问题的重要性。
- 同时，论文也强调了模型的潜在社会益处，如节能、更公平的多模态模型和提高模型预测的可解释性。

这篇论文在多模态模型领域提供了一种新的研究方向，通过检索增强的方法来提升模型的性能和效率，同时考虑了模型的伦理和社会影响。

## 阅读笔记

动机：此前的研究聚焦于使用RAG增强文本生成或这图像生成，都是单一模态的生成，这篇论文统一了两个任务，同时支持图像生成和文本生成

方法:

1. Clip作为基座，对两种模态的编码，具体结构见论文
2. CM3是一个可以同时完成图像生成和文本生成的 模型
3. 基于以上模型的选择，训练CM3生成模型

结论:统一了不同模态的检索和生成
